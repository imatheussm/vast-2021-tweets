{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vast Mini Challenge 03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to perform an exploratory analysis of the data from the _VAST Mini Challenge 03_, in order to answer the three questions proposed through data visualization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .csv and .geojson files of the Abila map were obtained from the conversion of the files provided by the mini challenge (present in the Abila folder) using the tool https://mapshaper.org. Just access the site, send the Abila folder with the 4 files inside and click export as .csv and .geojson. \n",
    "\n",
    "With the geojson obtained, the code from the **geocode.js** file was used, which loads the geojson file of the Abila map and the csv file of reports. With that, it was iterated about the locations of the reports and obtained their coordinates. After that, a .json file with the coordinates was obtained. This .json is called **coordinates.json**\n",
    "\n",
    "All these generated files are in the folder _abilaMap_processed_. \n",
    "\n",
    "**Attention**: it takes a few steps to run the js code used to generate the files. However, let's skip these steps and use the files already generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy as sp\n",
    "from datetime import datetime\n",
    "from nltk.corpus import stopwords\n",
    "# import plotly.express as px\n",
    "# import altair as alt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abila Map: Locations' Coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "The preprocessing includes punctuation and stopword removal and lemmatization. After that, calls ('ccdata' type) and posts ('mbdata' type) are separated into different files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the appropriate model\n",
    "#\n",
    "# Before doing so, they need to be installed. Choose one (or both, for safe measure) of them:\n",
    "#  - Bigger, slower, but more accurate: python -m spacy download en_core_web_trf\n",
    "#  - Small, faster, but less accurate: python -m spacy download en_core_web_sm\n",
    "nlp = sp.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] Processing data-frames...\n",
      " |_ [224/1033] Processing row...\r"
     ]
    }
   ],
   "source": [
    "# Loading and preprocessing the data sets.\n",
    "#\n",
    "# Time periods:\n",
    "#  - Period one: 1700-1830\n",
    "#  - Period two: 1831-2000\n",
    "#  - Period three: 2001-2131\n",
    "\n",
    "periods = (\"1700-1830\", \"1831-2000\", \"2001-2131\")\n",
    "data_frames = [pd.read_csv(f\"original_csv/csv-{period}.csv\") for period in periods]\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "for index, data_frame in enumerate(data_frames):\n",
    "    print(f\"[{index + 1}/{len(data_frames)}] Processing data-frames...\")\n",
    "    data_frame_length = data_frame.shape[0]\n",
    "\n",
    "    for row in range(data_frame_length):\n",
    "        print(f\" |_ [{row + 1}/{data_frame_length}] Processing row...\", end=\"\\r\")\n",
    "\n",
    "        message = data_frame.loc[row, \"message\"]\n",
    "        document = nlp(message)\n",
    "        tokens = [\n",
    "            token.lemma_ for token in document\n",
    "            if token.text not in stop_words  # Remove stop words\n",
    "            and token.is_punct is False      # Remove punctuation\n",
    "        ]\n",
    "\n",
    "        data_frame.loc[row, \"message\"] = \" \".join(tokens)\n",
    "\n",
    "    print(f\" |_ [{data_frame_length}/{data_frame_length}] Processing completed.\")\n",
    "\n",
    "combined_csv = pd.concat(data_frames)\n",
    "combined_csv.to_csv(\"processed/combined_csv.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate posts and calls (reports)\n",
    "combined_csv = pd.read_csv(\"processed/combined_csv.csv\")\n",
    "combined_csv['timestamp'] = combined_csv['date(yyyyMMddHHmmss)'].apply(lambda t : datetime.strptime(str(t),'%Y%m%d%H%M%S').strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "combined_csv = combined_csv.drop(['date(yyyyMMddHHmmss)'], axis='columns')\n",
    "\n",
    "posts = combined_csv[combined_csv['type'] == 'mbdata']\n",
    "\n",
    "posts.to_csv(\"processed/posts.csv\", index=False)\n",
    "\n",
    "reports = combined_csv[combined_csv['type'] == 'ccdata']\n",
    "\n",
    "reports = reports.drop(['author', 'longitude', 'latitude'], axis='columns')\\\n",
    "    .rename(columns={' location' : 'location'})\n",
    "\n",
    "reports.to_csv(\"processed/reports.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_csv(\"processed/posts.csv\") # tweets\n",
    "reports = pd.read_csv(\"processed/reports.csv\") # reports (emergency calls)\n",
    "combined_csv = pd.read_csv(\"processed/combined_csv.csv\") # csv containing the three periods (1700-1830, 1831-2000, 2001-2131)\n",
    "coordinates = pd.read_json(\"abilaMap_processed/coordinates.json\") # mapping from location to coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4063 entries, 0 to 4062\n",
      "Data columns (total 7 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   type                  4063 non-null   object \n",
      " 1   date(yyyyMMddHHmmss)  4063 non-null   int64  \n",
      " 2   author                3872 non-null   object \n",
      " 3   message               4062 non-null   object \n",
      " 4   latitude              147 non-null    float64\n",
      " 5   longitude             147 non-null    float64\n",
      " 6    location             176 non-null    object \n",
      "dtypes: float64(2), int64(1), object(4)\n",
      "memory usage: 222.3+ KB\n"
     ]
    }
   ],
   "source": [
    "combined_csv.info() # 147 non-null entries in latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in combined_csv.loc[~combined_csv.loc[:, \" location\"].isna()].index:\n",
    "    new_coordinates = coordinates.loc[coordinates.loc[:, \"location\"] == combined_csv.loc[i, \" location\"]]\n",
    "    \n",
    "    try:\n",
    "        new_longitude, new_latitude = new_coordinates.reset_index().iloc[0, -1]\n",
    "        combined_csv.loc[i, \"longitude\"], combined_csv.loc[i, \"latitude\"] = new_longitude, new_latitude\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "combined_csv = combined_csv.drop(' location', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4063 entries, 0 to 4062\n",
      "Data columns (total 6 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   type                  4063 non-null   object \n",
      " 1   date(yyyyMMddHHmmss)  4063 non-null   int64  \n",
      " 2   author                3872 non-null   object \n",
      " 3   message               4062 non-null   object \n",
      " 4   latitude              319 non-null    float64\n",
      " 5   longitude             319 non-null    float64\n",
      "dtypes: float64(2), int64(1), object(3)\n",
      "memory usage: 190.6+ KB\n"
     ]
    }
   ],
   "source": [
    "combined_csv.info() # now, there are 319 latitude and longitude entries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
